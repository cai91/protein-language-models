{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein Language Modeling with fairseq.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cai91/protein-language-models/blob/main/Protein_Language_Modeling_with_fairseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XilO7ArTAQyZ"
      },
      "source": [
        "# Protein Language Modeling with fairseq\n",
        "\n",
        "This is a quick walkthrough of how to set up a protein language modeling run using [pytorch/fairseq](https://github.com/pytorch/fairseq), an open-source langauge modeling toolkit developed by FAIR. Recently, fairseq was updated to allow directly training from fasta files, which makes setting up language modeling very easy!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KElj_kxDG9X"
      },
      "source": [
        "## What you'll need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74C1aBMDhAZ"
      },
      "source": [
        "### Packages\n",
        "\n",
        "*   numpy\n",
        "*   pytorch\n",
        "*   fairseq\n",
        "*   biopython (used in the tutorial to process fasta files)\n",
        "*   apex (optional, can speed up training)\n",
        "\n",
        "Colab comes with numpy and pytorch installed, and we won't be using apex in the tutorial, so we're only going to install biopython and fairseq. If you're training on your own devices you may need to install numpy + pytorch separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlTWB72qeSOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd735c6-3883-46f9-c036-7eaedc10f516"
      },
      "source": [
        "%%bash\n",
        "pushd . \n",
        "pip install biopython\n",
        "git clone -q https://github.com/pytorch/fairseq\n",
        "cd fairseq\n",
        "pip install -e .\n",
        "popd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content /content\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (4.41.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (1.14.3)\n",
            "Collecting hydra-core\n",
            "  Downloading https://files.pythonhosted.org/packages/79/03/fee705ef16675a103d8e929255f5fa0ee79432ac38bafad6935d6ad170f9/hydra_core-1.0.3-py3-none-any.whl (122kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (0.29.21)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (0.7)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (1.6.0+cu101)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+de97773) (0.5.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+de97773) (2.20)\n",
            "Collecting omegaconf>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq==1.0.0a0+de97773) (3.1.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==1.0.0a0+de97773) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.2->hydra-core->fairseq==1.0.0a0+de97773) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq==1.0.0a0+de97773) (3.3.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, PyYAML\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=f51d5e8c0ca738ae0873bcfc00914b5a855bf7d748f0da8629befbbef51da5eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for PyYAML (setup.py): started\n",
            "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=c85fc13527bfd65e6f2edf21ed9358c22b8ba66e364c53979f70ab5a399fc135\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built antlr4-python3-runtime PyYAML\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, portalocker, sacrebleu, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 fairseq hydra-core-1.0.3 omegaconf-2.0.3 portalocker-2.0.0 sacrebleu-1.4.14\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh1Nbee8wqCQ"
      },
      "source": [
        "#### Slightly Hacking Colab\n",
        "\n",
        "There is an issue with Colab and pip editable installs (see [this issue](https://github.com/pytorch/fairseq/issues/2407)). To work around this, we'll add the fairseq directory directly to `sys.path`. You can remove this step if you are following this tutorial outside of a notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKUyaTBSrEAo"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/fairseq\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWw6vqspDNeG"
      },
      "source": [
        "### Data\n",
        "For data, you're going to need a fasta file containing the set of sequences you want to train on. For the tutorial, we're going to use the 15051 sequences used to train [trRosetta](https://www.pnas.org/content/117/3/1496). This is a lot fewer sequences than you would actually want to use to train a language model, so this is just for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0qX_GeL5fcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fd64d7-69cd-480b-95f0-feca8f176ca3"
      },
      "source": [
        "%%bash\n",
        "pushd .\n",
        "mkdir -p data && cd data\n",
        "wget https://s3.amazonaws.com/proteindata/list15051.fasta -o /dev/null\n",
        "popd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content /content\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctYE2moKDYYL"
      },
      "source": [
        "### Compute\n",
        "Fairseq will automatically distribute to as many GPUs as you have available. If you want to limit the number of GPUs used, set the `CUDA_VISIBLE_DEVICES` environment variable. I often set only one visible device when debugging. \n",
        "\n",
        "With a bit more work it can also be set up to train on multiple nodes, but we won't be covering how to do that here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy_GAvqhD4UY"
      },
      "source": [
        "## Setting Things Up\n",
        "\n",
        "fairseq training requires three data files: `train.fasta`, `valid.fasta`, and `dict.txt`. The first two are simply training and validation splits of the full set of fasta sequences, while `dict.txt` is the dictionary that should be used to map tokens to integers. We'll set all three of these files up now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4JExDSeEeKG"
      },
      "source": [
        "### Training + Validation Splits\n",
        "\n",
        "We're just going to use a 95/5 random-split training and validation set. Depending on your problem setting, you may want to use specific sequence identity thresholds for holdouts, structural holdouts, or some other form of splitting. \n",
        "\n",
        "If your files are very large, this particular method of generating training + validation splits may be slow + memory intensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cux7ccvg6CHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2e5f22-1e6e-4a6d-d0bd-1d7527f4dd13"
      },
      "source": [
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "all_records = list(SeqIO.parse(\"data/list15051.fasta\", \"fasta\"))\n",
        "np.random.shuffle(all_records)\n",
        "valid_pct = 0.05\n",
        "num_valid_records = int(len(all_records) * valid_pct)\n",
        "valid_records = all_records[:num_valid_records]\n",
        "train_records = all_records[num_valid_records:]\n",
        "\n",
        "SeqIO.write(train_records, \"data/train.fasta\", \"fasta\")\n",
        "SeqIO.write(valid_records, \"data/valid.fasta\", \"fasta\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtl8Ve6uFUYI"
      },
      "source": [
        "### The Dictionary\n",
        "\n",
        "There are two ways to set up a dictionary, automatic and manual. Since these dictionaries are originally built for processing language, there are usually too many words to manually specify the dictionary. However in the case of protein sequences or nucleotide sequences, this is not necessarily true.\n",
        "\n",
        "The code below will automatically parse through the records loaded in the previous section to add amino acids to the dictionary. In addition, it will also add a count of the frequency that each token appears. This count is not used, but is a nice piece of metadata to have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXBVB2Zv9LZe"
      },
      "source": [
        "from fairseq.data import Dictionary\n",
        "from collections import Counter\n",
        "\n",
        "token_counter = Counter()\n",
        "for record in train_records:\n",
        "  token_counter.update(record.seq)\n",
        "\n",
        "dictionary = Dictionary()\n",
        "for token, count in sorted(token_counter.items()):\n",
        "  dictionary.add_symbol(token, count)\n",
        "\n",
        "with open(\"data/dict.txt\", \"w\") as f:\n",
        "  dictionary.save(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpPDjGICHAXO"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "And now you're ready to train a protein language model! The command for fairseq training is, well, `fairseq-train`. You can see a list of all options via `fairseq-train --help`. Let's just walk through the ones we're going to use below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDk0reubxy4J"
      },
      "source": [
        "### Anatomy of the fairseq-train command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzAQA7dHySsM"
      },
      "source": [
        "#### Basic Arguments\n",
        "\n",
        "There are a few basic arguments that you'll need to pass in to almost every `fairseq-train` command. \n",
        "\n",
        "*    `data`: This is actually a position argument, corresponding to the directory containing the training data files. In our case, this is going to be `./data`\n",
        "*    `--dataset-impl`: This tells fairseq what type of dataset to load. In our case, this is going to be `fasta`.\n",
        "*    `--task`: This is a fairseq construct that loads data, performs any necessary transformations, and logs metrics. To train with masked language modeling, we'll use the `masked_lm` task.\n",
        "*    `--criterion`: This is a fairseq construct that computes the loss and metrics given model outputs and targets. This can sometimes be swapped around, depending on the task you're interested in, but here we're just going to use the `masked_lm` criterion as well.\n",
        "*    `--arch`: This is the model architecture to use. Fairseq has several built-in model architectures, and you can also register your own. We're going to use the 12 layer `roberta_base` architecture.\n",
        "\n",
        "Check out the fairseq docs for a list of the dataset implementations, tasks, criteria, and architectures built in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2K2emuznI1"
      },
      "source": [
        "#### Training Arguments\n",
        "\n",
        "These are a set of common arguments that you'll want to set to train the model\n",
        "\n",
        "*    `--max-tokens`: The maximum number of tokens to feed into the model on any given batch. Fairseq uses adaptive batch sizes to allow larger batches with smaller sequences and smaller batches for larger sequences. Divide `--max-tokens` by the average sequence length in your dataset to get an approximate batch size. I usually set this to the maximum power of 2 that will fit on GPU.\n",
        "*    `--max-sentences`: This sets an actual maximum batch size - use this instead of `--max-tokens` if you want a fixed batch size.\n",
        "*    `--update-freq`: How many forward passes to run before taking a backwards pass. This lets you simulate a larger batch size. I typically debug runs with this set to 1, then increase it to simulate the batch size I actually want when I'm ready to train.\n",
        "*    `--lr`: The learning rate. Usually default to `1e-4`.\n",
        "*    `--optimizer`: The optimizer. Usually default to `adam`.\n",
        "*    `--lr-scheduler`: Learning rate scheduler. There are several built into fairseq, I usually use `inverse_sqrt`.\n",
        "*    `--warmup-updates`: How many learning rate warmup steps to use. Usually default to `16000` for large models + lots of data.\n",
        "*    `--max-positions`: The maximum number of positions to pass into the model. Will skip sequences longer than this.\n",
        "*    `--skip-invalid-size-inputs-valid-test`: If you set max positions, you will need to set this as well to skip those sequences during validation, otherwise fairseq will complain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahn2--H-19cD"
      },
      "source": [
        "#### Apex\n",
        "\n",
        "These arguments require [nvidia/apex](https://github.com/nvidia/apex) to be installed.\n",
        "\n",
        "*   `--fp16`: Use half precision (speeds up training if GPU supports it, reduces memory).\n",
        "*   `--fp16-init-scale`: Initial fp16 loss scale. I usually default to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjSMGFs2aV2"
      },
      "source": [
        "#### Saving Arguments\n",
        "\n",
        "*   `--validate-interval-updates`: Fairseq by default will run the validation pass at the end of each epoch, but this can be very infrequent if you have a large dataset. This allows you to also run the validation pass after a certain number of updates.\n",
        "*   `--save-interval-updates`: Fairseq by default will save a checkpoint at the end of each epoch, but this can be very infrequent if you have a large dataset. This allows you to also save a checkpoint after a certain number of updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdD2DPQ3IkV"
      },
      "source": [
        "### Running the Training\n",
        "\n",
        "Below is the full fairseq-train command with some defaults filled in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HBjspxl_LOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbcc093-8d57-4630-bfc9-04f313356efd"
      },
      "source": [
        "!fairseq-train \\\n",
        "\n",
        "  # Basic Arguments\n",
        "  ./data/ \\\n",
        "  --dataset-impl fasta \\\n",
        "  --task masked_lm \\\n",
        "  --criterion masked_lm \\\n",
        "  --arch roberta_base \\\n",
        "\n",
        "  # Training Arguments\n",
        "  --max-tokens 8096 \\\n",
        "  --update-freq 4 \\\n",
        "  --lr 1e-4 \\\n",
        "  --optimizer adam \\\n",
        "  --lr-scheduler inverse_sqrt \\\n",
        "  --warmup-updates 16000 \\\n",
        "  --max-positions 1024 \\\n",
        "  --skip-invalid-size-inputs-valid-test \\\n",
        "\n",
        "  # Requires Apex\n",
        "  # --fp16 \\\n",
        "  # --fp16-init-scale 4 \\\n",
        "\n",
        "  # Saving Arguments\n",
        "  --validate-interval-updates 5000 \\\n",
        "  --save-interval-updates 5000 \\\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/hydra/_internal/hydra.py:71: UserWarning: \n",
            "@hydra.main(strict) flag is deprecated and will removed in the next version.\n",
            "See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/strict_mode_flag_deprecated\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "2020-11-03 20:13:55 | INFO | fairseq_cli.train | {'common': {'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'memory_efficient_bf16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False}, 'distributed_training': {'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'local_rank': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'distributed_world_size': 1, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8096, 'batch_size': None, 'required_batch_size_multiple': 8, 'dataset_impl': 'fasta', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': False, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'max_tokens_valid': 8096, 'batch_size_valid': None, 'required_seq_len_multiple': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0}, 'optimization': {'max_epoch': 0, 'max_update': 0, 'clip_norm': 25.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0001], 'min_lr': -1.0, 'use_bmuf': False, 'stop_time_hours': 0}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'finetune_from_model': None, 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'block_lr': 1, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'task': Namespace(_name='masked_lm', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='masked_lm', curriculum=0, data='./', data_buffer_size=10, dataset_impl='fasta', ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freq_weighted_replacement=False, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, leave_unmasked_prob=0.1, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', mask_prob=0.15, mask_whole_words=False, max_epoch=0, max_positions=1024, max_tokens=8096, max_tokens_valid=8096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='checkpoints', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_time_hours=0, task='masked_lm', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[16], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, warmup_init_lr=-1, warmup_updates=16000, weight_decay=0.0, zero_sharding='none'), 'optimizer': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, '_name': 'adam', 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'warmup_updates': 16000, 'warmup_init_lr': -1, '_name': 'inverse_sqrt', 'lr': [0.0001]}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'generation': {'beam': 5, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'tokenizer': None, 'bpe': None, 'criterion': Namespace(_name='masked_lm', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='masked_lm', curriculum=0, data='./', data_buffer_size=10, dataset_impl='fasta', ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freq_weighted_replacement=False, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, leave_unmasked_prob=0.1, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', mask_prob=0.15, mask_whole_words=False, max_epoch=0, max_positions=1024, max_tokens=8096, max_tokens_valid=8096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='checkpoints', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_time_hours=0, task='masked_lm', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[16], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, warmup_init_lr=-1, warmup_updates=16000, weight_decay=0.0, zero_sharding='none'), 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='masked_lm', curriculum=0, data='./', data_buffer_size=10, dataset_impl='fasta', ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freq_weighted_replacement=False, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, leave_unmasked_prob=0.1, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', mask_prob=0.15, mask_whole_words=False, max_epoch=0, max_positions=1024, max_tokens=8096, max_tokens_valid=8096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='checkpoints', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_time_hours=0, task='masked_lm', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[16], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, warmup_init_lr=-1, warmup_updates=16000, weight_decay=0.0, zero_sharding='none')}\n",
            "2020-11-03 20:13:55 | INFO | fairseq.tasks.masked_lm | dictionary: 24 types\n",
            "100% 191k/191k [00:00<00:00, 60.3MB/s]\n",
            "100% 191k/191k [00:00<00:00, 68.1MB/s]\n",
            "2020-11-03 20:13:55 | INFO | fairseq.data.data_utils | loaded 752 examples from: ./valid\n",
            "2020-11-03 20:13:55 | INFO | fairseq.tasks.masked_lm | loaded 449 blocks from: ./valid\n",
            "2020-11-03 20:13:58 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(25, 768, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict()\n",
            ")\n",
            "2020-11-03 20:13:58 | INFO | fairseq_cli.train | task: masked_lm (MaskedLMTask)\n",
            "2020-11-03 20:13:58 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2020-11-03 20:13:58 | INFO | fairseq_cli.train | criterion: masked_lm (MaskedLmLoss)\n",
            "2020-11-03 20:13:58 | INFO | fairseq_cli.train | num. model params: 86455321 (num. trained: 86455321)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 349, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed_utils.py\", line 317, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 96, in main\n",
            "    trainer = Trainer(cfg, task, model, criterion, quantizer)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 78, in __init__\n",
            "    self._model = self._model.to(device=self.device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 607, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 354, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 354, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 354, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 376, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 605, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}